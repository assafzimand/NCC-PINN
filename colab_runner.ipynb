{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCC-PINN - Colab GPU Runner\n",
    "\n",
    "This notebook trains Physics-Informed Neural Networks (PINNs) and analyzes internal representations with Nearest Class Centroid (NCC) on Google Colab GPU.\n",
    "\n",
    "**Features:**\n",
    "- Mounts Google Drive for persistent storage\n",
    "- Clones the NCC-PINN repository from GitHub\n",
    "- Installs dependencies with CUDA support\n",
    "- Runs training + NCC analysis pipeline\n",
    "- Auto-generates datasets if missing\n",
    "- Auto-saves all outputs (plots, metrics, checkpoints) to Drive\n",
    "\n",
    "**Runtime:** GPU (T4 or better recommended)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "All outputs will be saved to: `/content/drive/MyDrive/NCC-PINN/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory on Drive\n",
    "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/NCC-PINN'\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Drive mounted. Outputs will be saved to: {DRIVE_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_URL = \"https://github.com/assafzimand/NCC-PINN.git\"\n",
    "REPO_NAME = \"NCC-PINN\"\n",
    "WORK_DIR = f\"/content/{REPO_NAME}\"\n",
    "\n",
    "# Clone or update repository\n",
    "if os.path.exists(WORK_DIR):\n",
    "    print(f\"Repository already exists at {WORK_DIR}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd {WORK_DIR} && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} {WORK_DIR}\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"\\n✓ Working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA for Colab (CUDA 12.1)\n",
    "print(\"Installing PyTorch with CUDA support...\")\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 --quiet\n",
    "\n",
    "print(\"\\nInstalling project dependencies...\")\n",
    "%pip install -r requirements.txt --quiet\n",
    "\n",
    "# Verify CUDA is available\n",
    "import torch\n",
    "print(f\"\\n✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"✓ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: CUDA not available! Training will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Optional) View Configuration\n",
    "\n",
    "View the current training configuration before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current configuration\n",
    "!cat config/config.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training + NCC Analysis & Auto-Save to Drive\n",
    "\n",
    "**This cell does everything:**\n",
    "1. Generates datasets if they don't exist\n",
    "2. Trains the PINN model\n",
    "3. Runs NCC analysis\n",
    "4. Saves all results to Google Drive\n",
    "\n",
    "**Note:** This may take 10-30 minutes depending on configuration and GPU type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NCC-PINN: Training + NCC Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run the complete pipeline (dataset gen + training + NCC)\n",
    "!python run_ncc.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Pipeline Complete! Saving results to Google Drive...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create timestamped backup folder on Drive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_dir = Path(DRIVE_OUTPUT_DIR) / f\"run_{timestamp}\"\n",
    "backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving to: {backup_dir}\\n\")\n",
    "\n",
    "# 1. Copy outputs directory (training plots, NCC plots, metrics, summary)\n",
    "outputs_dir = Path(\"outputs\")\n",
    "if outputs_dir.exists():\n",
    "    print(\"Copying outputs (training plots, NCC plots, metrics)...\")\n",
    "    for problem_dir in outputs_dir.iterdir():\n",
    "        if problem_dir.is_dir():\n",
    "            dest = backup_dir / \"outputs\" / problem_dir.name\n",
    "            shutil.copytree(problem_dir, dest, dirs_exist_ok=True)\n",
    "            print(f\"  ✓ Copied {problem_dir.name}\")\n",
    "else:\n",
    "    print(\"⚠ No outputs directory found\")\n",
    "\n",
    "# 2. Copy checkpoints directory (best_model.pt, final_model.pt, etc.)\n",
    "checkpoints_dir = Path(\"checkpoints\")\n",
    "if checkpoints_dir.exists():\n",
    "    print(\"\\nCopying checkpoints...\")\n",
    "    for problem_dir in checkpoints_dir.iterdir():\n",
    "        if problem_dir.is_dir():\n",
    "            dest = backup_dir / \"checkpoints\" / problem_dir.name\n",
    "            shutil.copytree(problem_dir, dest, dirs_exist_ok=True)\n",
    "            print(f\"  ✓ Copied {problem_dir.name}\")\n",
    "else:\n",
    "    print(\"⚠ No checkpoints directory found\")\n",
    "\n",
    "# 3. Copy dataset visualizations ONLY (not the .pt data files)\n",
    "datasets_dir = Path(\"datasets\")\n",
    "if datasets_dir.exists():\n",
    "    print(\"\\nCopying dataset visualizations...\")\n",
    "    for problem_dir in datasets_dir.iterdir():\n",
    "        if problem_dir.is_dir():\n",
    "            dest = backup_dir / \"datasets\" / problem_dir.name\n",
    "            dest.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Copy only visualization files (PNG, not .pt data files)\n",
    "            for viz_file in problem_dir.glob(\"*.png\"):\n",
    "                shutil.copy2(viz_file, dest / viz_file.name)\n",
    "                print(f\"  ✓ Copied {problem_dir.name}/{viz_file.name}\")\n",
    "else:\n",
    "    print(\"⚠ No datasets directory found\")\n",
    "\n",
    "# 4. Copy the configuration used\n",
    "config_file = Path(\"config/config.yaml\")\n",
    "if config_file.exists():\n",
    "    print(\"\\nCopying configuration...\")\n",
    "    dest_config = backup_dir / \"config.yaml\"\n",
    "    shutil.copy2(config_file, dest_config)\n",
    "    print(f\"  ✓ Copied config.yaml\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ All results saved to Google Drive!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nLocation: MyDrive/NCC-PINN/run_{timestamp}/\")\n",
    "print(f\"\\nSaved structure:\")\n",
    "print(f\"  - outputs/<problem>/ (training plots, NCC plots, metrics.json, summary.txt)\")\n",
    "print(f\"  - checkpoints/<problem>/ (best_model.pt, final_model.pt, etc.)\")\n",
    "print(f\"  - datasets/<problem>/ (dataset visualizations only)\")\n",
    "print(f\"  - config.yaml (configuration used for this run)\")\n",
    "print(f\"\\n{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Training Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import json\n",
    "\n",
    "# Find the output directory\n",
    "outputs_dir = Path(\"outputs\")\n",
    "problem_dirs = [d for d in outputs_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "if problem_dirs:\n",
    "    latest_run = problem_dirs[0]  # Assuming single run\n",
    "    print(f\"Displaying results from: {latest_run.name}\\n\")\n",
    "    \n",
    "    # Load and display metrics\n",
    "    metrics_file = latest_run / \"metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Training Metrics Summary\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total epochs: {len(metrics.get('train_loss_epochs', []))}\") \n",
    "        if metrics.get('train_loss'):\n",
    "            print(f\"Final train loss: {metrics['train_loss'][-1]:.6f}\")\n",
    "        if metrics.get('eval_loss'):\n",
    "            print(f\"Final eval loss: {metrics['eval_loss'][-1]:.6f}\")\n",
    "        if metrics.get('train_rel_l2'):\n",
    "            print(f\"Final train rel-L2: {metrics['train_rel_l2'][-1]:.6f}\")\n",
    "        if metrics.get('eval_rel_l2'):\n",
    "            print(f\"Final eval rel-L2: {metrics['eval_rel_l2'][-1]:.6f}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Display training curves\n",
    "    training_plots_dir = latest_run / \"training_plots\"\n",
    "    if training_plots_dir.exists():\n",
    "        training_curves = training_plots_dir / \"training_curves.png\"\n",
    "        if training_curves.exists():\n",
    "            print(\"Training Curves:\")\n",
    "            display(Image(filename=str(training_curves)))\n",
    "    \n",
    "    # Display NCC accuracy plot\n",
    "    ncc_plots_dir = latest_run / \"ncc_plots\"\n",
    "    if ncc_plots_dir.exists():\n",
    "        ncc_accuracy = ncc_plots_dir / \"ncc_layer_accuracy.png\"\n",
    "        if ncc_accuracy.exists():\n",
    "            print(\"\\nNCC Layer Accuracy:\")\n",
    "            display(Image(filename=str(ncc_accuracy)))\n",
    "        \n",
    "        ncc_compactness = ncc_plots_dir / \"ncc_compactness.png\"\n",
    "        if ncc_compactness.exists():\n",
    "            print(\"\\nNCC Compactness:\")\n",
    "            display(Image(filename=str(ncc_compactness)))\n",
    "else:\n",
    "    print(\"No output directories found. Make sure training completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Dataset Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Find dataset visualizations\n",
    "datasets_dir = Path(\"datasets\")\n",
    "problem_dirs = [d for d in datasets_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "if problem_dirs:\n",
    "    for problem_dir in problem_dirs:\n",
    "        print(f\"Dataset: {problem_dir.name}\\n\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Show dataset visualization plots\n",
    "        for viz_file in sorted(problem_dir.glob(\"*.png\")):\n",
    "            print(f\"\\n{viz_file.name}:\")\n",
    "            display(Image(filename=str(viz_file)))\n",
    "else:\n",
    "    print(\"No dataset visualizations found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Tips\n",
    "\n",
    "---\n",
    "\n",
    "### Configuration\n",
    "- Edit `config/config.yaml` in the repository to customize:\n",
    "  - Problem to solve (`schrodinger`, `wave1d, burgers1d, burgers2d`)\n",
    "  - Architecture (layer sizes)\n",
    "  - Learning rate, batch size, epochs\n",
    "  - Loss weights (residual, IC, BC)\n",
    "  - NCC bins\n",
    "\n",
    "### Pipeline Steps\n",
    "1. **Dataset Generation**: `run_ncc.py` checks if datasets exist and generates them if missing\n",
    "2. **Training**: Trains PINN model with physics-informed loss\n",
    "3. **NCC Analysis**: Analyzes internal representations across all hidden layers\n",
    "\n",
    "### Outputs\n",
    "- **`outputs/<problem>/`**:\n",
    "  - `training_plots/`: Loss curves and predictions\n",
    "  - `ncc_plots/`: NCC accuracy, compactness, geometry, margins, confusion matrices\n",
    "  - `metrics.json`: Training metrics (loss, rel-L2 error)\n",
    "  - `summary.txt`: Training summary\n",
    "  - Problem-specific visualizations (e.g., Schrödinger heatmaps)\n",
    "  \n",
    "- **`checkpoints/<problem>/`**: Saved model checkpoints\n",
    "- **`datasets/<problem>/`**: Dataset visualizations\n",
    "\n",
    "### Tips\n",
    "- **Use GPU runtime**: Runtime → Change runtime type → GPU (T4 recommended)\n",
    "- **Long runs**: Keep browser tab open or use Colab Pro for extended sessions\n",
    "- **Results persist**: All results are auto-saved to Drive with timestamp\n",
    "- **Resume training**: Use `resume_from` in config to continue from checkpoint\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlG0LQAD-JVI"
   },
   "source": [
    "# Schrödinger PINN - Grid Search on Colab GPU\n",
    "\n",
    "This notebook runs hyperparameter grid search for the Schrödinger equation PINN on Google Colab GPU.\n",
    "\n",
    "**Features:**\n",
    "- Mounts Google Drive for persistent storage\n",
    "- Clones the repository from Git\n",
    "- Installs dependencies with CUDA support\n",
    "- Runs grid search with MLflow tracking\n",
    "- Auto-saves all outputs (plots, evaluations, checkpoints) to Drive\n",
    "\n",
    "**Runtime:** GPU (T4 or better recommended)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u-56nZe-JVP"
   },
   "source": [
    "## 1. Check GPU Availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1761724662407,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "WMZj-H8h-JVQ",
    "outputId": "4c3942af-a466-4b38-d130-5bdad44db3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 29 07:57:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   67C    P8             15W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnjZ2cUY-JVS"
   },
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "All outputs will be saved to: `/content/drive/MyDrive/Schrodinger_PINN/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17907,
     "status": "ok",
     "timestamp": 1761724682953,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "PzCPNGtv-JVT",
    "outputId": "abc3961d-fcf2-4f9f-9386-5a7bcd3b7d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      "✓ Drive mounted. Outputs will be saved to: /content/drive/MyDrive/Schrodinger_PINN\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory on Drive\n",
    "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Schrodinger_PINN'\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Drive mounted. Outputs will be saved to: {DRIVE_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpzXQgtl-JVW"
   },
   "source": [
    "## 3. Clone Repository\n",
    "\n",
    "**⚠️ IMPORTANT:** Update the Git repository URL below with your actual repo URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3541,
     "status": "ok",
     "timestamp": 1761724691984,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "OZRDzQYu-JVX",
    "outputId": "33a1172a-47cf-4b25-da1b-a0f1a1032ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository from https://github.com/assafzimand/Schrodinger-Equation.git...\n",
      "Cloning into '/content/Schrodinger_Equation'...\n",
      "remote: Enumerating objects: 1635, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 1635 (delta 0), reused 0 (delta 0), pack-reused 1623 (from 3)\u001b[K\n",
      "Receiving objects: 100% (1635/1635), 49.05 MiB | 21.66 MiB/s, done.\n",
      "Resolving deltas: 100% (115/115), done.\n",
      "\n",
      "✓ Working directory: /content/Schrodinger_Equation\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "REPO_URL = \"https://github.com/assafzimand/Schrodinger-Equation.git\"  # ⚠️ UPDATE THIS\n",
    "REPO_NAME = \"Schrodinger_Equation\"\n",
    "WORK_DIR = f\"/content/{REPO_NAME}\"\n",
    "\n",
    "# Clone or update repository\n",
    "if os.path.exists(WORK_DIR):\n",
    "    print(f\"Repository already exists at {WORK_DIR}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd {WORK_DIR} && git pull\n",
    "else:\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL} {WORK_DIR}\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"\\n✓ Working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6id8cF3y-JVX"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21167,
     "status": "ok",
     "timestamp": 1761724721188,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "d2iLebbf-JVb",
    "outputId": "d22ecdf8-596d-4488-b095-d3252c3f170b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing PyTorch with CUDA support...\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "Installing other dependencies...\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting mlflow-skinny==3.5.1 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.5.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.5.1 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow)\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.0)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pyarrow<22,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.44)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (8.3.0)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.5.1->mlflow)\n",
      "  Downloading databricks_sdk-0.70.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (0.120.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (3.1.45)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (1.37.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (2.11.10)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (1.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (2.32.4)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (4.15.0)\n",
      "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.5.1->mlflow) (0.38.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow) (2.38.0)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.5.1->mlflow) (0.48.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.5.1->mlflow) (0.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.5.1->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.5.1->mlflow) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.5.1->mlflow) (0.58b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.5.1->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.5.1->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.5.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.5.1->mlflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.5.1->mlflow) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.5.1->mlflow) (2025.10.5)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.5.1->mlflow) (0.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.5.1->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow) (4.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.5.1->mlflow) (4.11.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.5.1->mlflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.5.1->mlflow) (0.6.1)\n",
      "Downloading mlflow-3.5.1-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-3.5.1-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_tracing-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sdk-0.70.0-py3-none-any.whl (752 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: gunicorn, graphql-core, graphql-relay, docker, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-CORS-6.0.1 databricks-sdk-0.70.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.5.1 mlflow-skinny-3.5.1 mlflow-tracing-3.5.1\n",
      "\n",
      "✓ PyTorch version: 2.8.0+cu126\n",
      "✓ CUDA available: True\n",
      "✓ CUDA version: 12.6\n",
      "✓ GPU device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch with CUDA for Colab (usually CUDA 12.x)\n",
    "print(\"Installing PyTorch with CUDA support...\")\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "print(\"\\nInstalling other dependencies...\")\n",
    "%pip install numpy scipy matplotlib seaborn PyYAML mlflow pandas\n",
    "\n",
    "# Verify CUDA is available\n",
    "import torch\n",
    "print(f\"\\n✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"✓ GPU device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocen8dXp-JVc"
   },
   "source": [
    "## 5. Generate Training and Evaluation Datasets\n",
    "\n",
    "Generate the datasets if they don't exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761724734924,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "Eg6vTjLc-JVd",
    "outputId": "344422d3-6120-4b87-a67f-7ff2219a17ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training dataset exists: data/processed/dataset.npz\n",
      "✓ Evaluation dataset exists: data/processed/dataset_eval.npz\n",
      "\n",
      "✓ Datasets ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if datasets exist\n",
    "train_dataset_path = \"data/processed/dataset.npz\"\n",
    "eval_dataset_path = \"data/processed/dataset_eval.npz\"\n",
    "\n",
    "if not os.path.exists(train_dataset_path):\n",
    "    print(\"Generating training dataset...\")\n",
    "    !python src/data/generate_dataset.py\n",
    "else:\n",
    "    print(f\"✓ Training dataset exists: {train_dataset_path}\")\n",
    "\n",
    "if not os.path.exists(eval_dataset_path):\n",
    "    print(\"\\nGenerating evaluation dataset...\")\n",
    "    !python src/data/generate_eval_dataset.py\n",
    "else:\n",
    "    print(f\"✓ Evaluation dataset exists: {eval_dataset_path}\")\n",
    "\n",
    "print(\"\\n✓ Datasets ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZjYu1SV-JVd"
   },
   "source": [
    "## 6. Configure Grid Search\n",
    "\n",
    "View current grid search configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1761724748432,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "DBe-gUOt-JVd",
    "outputId": "cce47b3c-bba5-4116-a7f6-80e6d93be071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Grid Search Configuration\n",
      "# Searches over loss weights and batch sizes to find optimal hyperparameters\n",
      "\n",
      "grid_search:\n",
      "  # Base configuration (will be overridden by grid parameters)\n",
      "  base_config: config/train.yaml\n",
      "  \n",
      "  # Number of epochs for each run\n",
      "  epochs: 20\n",
      "  \n",
      "  # Grid parameters to search\n",
      "  parameters:\n",
      "    # Initial condition loss weight\n",
      "    # Based on analysis: 1.0 (paper), 10, 20 (moderate boost)\n",
      "    weight_initial: [400]\n",
      "    \n",
      "    # Boundary condition weight (keep constant)\n",
      "    weight_boundary: [1]\n",
      "    \n",
      "    # PDE residual weight (try boosting to improve time evolution)\n",
      "    weight_residual: [400]\n",
      "    \n",
      "    # Batch size (affects gradient noise and convergence)\n",
      "    batch_size: [512]\n",
      "  \n",
      "  # MLflow experiment name for grid search\n",
      "  experiment_name: schrodinger_grid_search\n",
      "  \n",
      "  # Save results summary\n",
      "  results_file: outputs/grid_search_results.csv\n",
      "  \n",
      "  # Early stopping criteria (optional)\n",
      "  early_stopping:\n",
      "    enabled: false\n",
      "    patience: 100\n",
      "    min_delta: 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display current grid search configuration\n",
    "!cat config/grid_search.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w80Va5Cd-JVd"
   },
   "source": [
    "### Optional: Modify Grid Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1761724756485,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "4LAarR_F-JVe"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Example: Modify grid search parameters (uncomment to use)\n",
    "# config_path = 'config/grid_search.yaml'\n",
    "# with open(config_path, 'r') as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "#\n",
    "# # Modify parameters\n",
    "# config['grid_search']['epochs'] = 100\n",
    "# config['grid_search']['parameters']['weight_initial'] = [1, 10, 100]\n",
    "# config['grid_search']['parameters']['weight_residual'] = [1, 10, 100]\n",
    "# config['grid_search']['parameters']['batch_size'] = [512, 1024]\n",
    "#\n",
    "# # Save modified config\n",
    "# with open(config_path, 'w') as f:\n",
    "#     yaml.dump(config, f, default_flow_style=False)\n",
    "#\n",
    "# print(\"✓ Grid search config updated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Grid Search & Auto-Save to Drive\n",
    "\n",
    "This cell runs the grid search and automatically saves results to Drive when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60775,
     "status": "ok",
     "timestamp": 1761724821919,
     "user": {
      "displayName": "אסף צימנד",
      "userId": "11280149340557833545"
     },
     "user_tz": -120
    },
    "id": "j2hgd56g-JVf",
    "outputId": "5899f23c-139d-4a61-be73-612150fab61d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH - Schrödinger PINN Hyperparameter Optimization\n",
      "================================================================================\n",
      "\n",
      "Grid Configuration:\n",
      "  Total combinations: 1\n",
      "  Epochs per run: 20\n",
      "  Base config: config/train.yaml\n",
      "  Parameters:\n",
      "    weight_initial: [400]\n",
      "    weight_boundary: [1]\n",
      "    weight_residual: [400]\n",
      "    batch_size: [512]\n",
      "\n",
      "  Estimated time per run: 1.3 minutes\n",
      "  Estimated total time: 0.0 hours\n",
      "\n",
      "[Auto-Confirm] Proceeding with 1 runs.\n",
      "2025/10/29 07:59:27 INFO mlflow.tracking.fluent: Experiment with name 'schrodinger_grid_search' does not exist. Creating a new experiment.\n",
      "\n",
      "================================================================================\n",
      "Run 1/1\n",
      "  weight_initial=400\n",
      "  weight_residual=400\n",
      "  batch_size=512\n",
      "================================================================================\n",
      "======================================================================\n",
      "Full Training Run - Schrödinger PINN\n",
      "======================================================================\n",
      "\n",
      "Device: cuda\n",
      "GPU: NVIDIA L4\n",
      "\n",
      "Loading training dataset: data/processed/dataset.npz\n",
      "  Total collocation: 20000\n",
      "  Initial: 50\n",
      "  Boundary: 100\n",
      "\n",
      "Using ENTIRE dataset for training (no split)...\n",
      "\n",
      "Loading evaluation dataset: data/processed/dataset_eval.npz\n",
      "  Eval collocation: 5000\n",
      "  Eval initial: 15\n",
      "  Eval boundary: 30\n",
      "\n",
      "Creating model...\n",
      "  Parameters: 40,902\n",
      "  Loss weights: IC=400, BC=1, Residual=400\n",
      "  Batch size: 512\n",
      "  Train batches/epoch: 40\n",
      "\n",
      "MLflow run ID: 694e31d222cc4797bc44f7c16e38f2b7\n",
      "\n",
      "Training for 20 epochs...\n",
      "----------------------------------------------------------------------\n",
      "/content/Schrodinger_Equation/train_full.py:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
      "/content/Schrodinger_Equation/train_full.py:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "/content/Schrodinger_Equation/src/model/schrodinger_model.py:171: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at /pytorch/aten/src/ATen/EmptyTensor.cpp:55.)\n",
      "  h = torch.complex(u, v)\n",
      "Epoch    1/20 | Loss: 194.496126 | L²: 0.889008 | t_fwd=0.05s, t_bwd=0.04s, t_step=0.00s | batches=4.07s, l2_prep=0.27s, l2_pred=0.08s, mlflow=0.00s | epoch=5.2s | Total: 5.2s\n",
      "Epoch    2/20 | Loss: 43.398269 | L²: 0.923983 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.93s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.2s | Total: 7.4s\n",
      "Epoch    3/20 | Loss: 32.528417 | L²: 0.937480 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.77s, l2_prep=0.30s, l2_pred=0.00s, mlflow=0.00s | epoch=2.1s | Total: 9.5s\n",
      "  ✓ New best eval/relative_l2_error: 1.005915 at epoch 4 → saved best_model.pt\n",
      "Epoch    4/20 | Loss: 24.927787 | L²: 0.981637 | t_fwd=0.02s, t_bwd=0.01s, t_step=0.00s | batches=1.71s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.1s | Total: 11.6s\n",
      "Epoch    5/20 | Loss: 23.340160 | L²: 0.969447 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.76s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 13.6s\n",
      "Epoch    6/20 | Loss: 23.702742 | L²: 0.976030 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.73s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 15.6s\n",
      "Epoch    7/20 | Loss: 20.730025 | L²: 0.990410 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.73s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 17.6s\n",
      "Epoch    8/20 | Loss: 21.347531 | L²: 0.984775 | t_fwd=0.02s, t_bwd=0.01s, t_step=0.00s | batches=1.87s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.2s | Total: 19.8s\n",
      "Epoch    9/20 | Loss: 20.382694 | L²: 0.992355 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.71s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 21.8s\n",
      "Epoch   10/20 | Loss: 19.561062 | L²: 0.999504 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.76s, l2_prep=0.45s, l2_pred=0.00s, mlflow=0.00s | epoch=2.2s | Total: 24.0s\n",
      "Epoch   11/20 | Loss: 19.381194 | L²: 0.996623 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.75s, l2_prep=0.28s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 26.1s\n",
      "Epoch   12/20 | Loss: 19.748109 | L²: 1.009199 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.91s, l2_prep=0.28s, l2_pred=0.00s, mlflow=0.00s | epoch=2.3s | Total: 28.4s\n",
      "Epoch   13/20 | Loss: 20.643572 | L²: 0.996186 | t_fwd=0.02s, t_bwd=0.01s, t_step=0.00s | batches=1.69s, l2_prep=0.30s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 30.4s\n",
      "Epoch   14/20 | Loss: 18.624832 | L²: 1.001407 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.70s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 32.3s\n",
      "Epoch   15/20 | Loss: 18.671686 | L²: 1.012749 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.73s, l2_prep=0.49s, l2_pred=0.00s, mlflow=0.00s | epoch=2.2s | Total: 34.6s\n",
      "Epoch   16/20 | Loss: 18.614762 | L²: 1.003215 | t_fwd=0.02s, t_bwd=0.01s, t_step=0.00s | batches=1.70s, l2_prep=0.28s, l2_pred=0.00s, mlflow=0.00s | epoch=2.1s | Total: 36.6s\n",
      "Epoch   17/20 | Loss: 17.772423 | L²: 1.000870 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.94s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.2s | Total: 38.8s\n",
      "Epoch   18/20 | Loss: 18.467893 | L²: 1.011703 | t_fwd=0.02s, t_bwd=0.01s, t_step=0.00s | batches=1.70s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 40.8s\n",
      "Epoch   19/20 | Loss: 18.652004 | L²: 1.017000 | t_fwd=0.02s, t_bwd=0.02s, t_step=0.00s | batches=1.71s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 42.8s\n",
      "Epoch   20/20 | Loss: 17.712321 | L²: 1.027091 | t_fwd=0.02s, t_bwd=0.01s, t_step=0.00s | batches=1.69s, l2_prep=0.27s, l2_pred=0.00s, mlflow=0.00s | epoch=2.0s | Total: 44.8s\n",
      "\n",
      "Generating loss curves...\n",
      "Saved: outputs/plots/694e31d222cc4797bc44f7c16e38f2b7/training_curves.png\n",
      "  ✓ Loss curves saved to: outputs/plots/694e31d222cc4797bc44f7c16e38f2b7\n",
      "\n",
      "Generating training evolution plot...\n",
      "  ✓ Evolution plot saved to: outputs/plots/694e31d222cc4797bc44f7c16e38f2b7/training_evolution.png\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training completed in 0.7 minutes\n",
      "Final train loss: 17.712321\n",
      "Final train L² error: 1.027091\n",
      "Final eval L² error: 1.048979\n",
      "\n",
      "All artifacts saved to MLflow run: 694e31d222cc4797bc44f7c16e38f2b7\n",
      "Loss curves: outputs/plots/694e31d222cc4797bc44f7c16e38f2b7\n",
      "Checkpoints: outputs/checkpoints\n",
      "\n",
      "======================================================================\n",
      "Running Automatic Evaluation on Final Model\n",
      "======================================================================\n",
      "Evaluating best checkpoint (based on eval/relative_l2_error): outputs/checkpoints/best_model.pt\n",
      "======================================================================\n",
      "Model Evaluation\n",
      "======================================================================\n",
      "\n",
      "Checkpoint: outputs/checkpoints/best_model.pt\n",
      "Device: cuda\n",
      "\n",
      "1. Loading model...\n",
      "\n",
      "✗ Evaluation failed: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL src.config_loader.Config was not an allowed global by default. Please use `torch.serialization.add_safe_globals([src.config_loader.Config])` or the `torch.serialization.safe_globals([src.config_loader.Config])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "  You can run evaluation manually later.\n",
      "======================================================================\n",
      "\n",
      "✓ Run 1 completed | Run ID: 694e31d222cc4797bc44f7c16e38f2b7\n",
      "\n",
      "================================================================================\n",
      "Grid Search Complete! (0.01 hours)\n",
      "================================================================================\n",
      "\n",
      "Results saved to: outputs/grid_search_results.csv\n",
      "\n",
      "Top 5 Configurations (by validation loss):\n",
      "--------------------------------------------------------------------------------\n",
      "No successful runs with metrics.\n",
      "\n",
      "================================================================================\n",
      "All artifacts (plots, evaluations) saved per run ID\n",
      "  Training evolution: outputs/plots/{run_id}/training_evolution.png\n",
      "  Loss curves: outputs/plots/{run_id}/\n",
      "  Evaluation: outputs/evaluation/{run_id}/\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "\n",
    "# Run grid search with auto-confirmation\n",
    "!python grid_search.py --yes\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Grid Search Complete! Saving results to Google Drive...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get the most recent MLflow run IDs from the grid search experiment\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"schrodinger_grid_search\")\n",
    "\n",
    "if experiment:\n",
    "    # Get all runs from this experiment\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "    )\n",
    "    \n",
    "    # Create timestamped backup folder on Drive\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_dir = Path(DRIVE_OUTPUT_DIR) / f\"grid_search_{timestamp}\"\n",
    "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nSaving to: {backup_dir}\\n\")\n",
    "    \n",
    "    # Copy grid search summary\n",
    "    results_csv = Path(\"outputs/grid_search_results.csv\")\n",
    "    if results_csv.exists():\n",
    "        shutil.copy2(results_csv, backup_dir / \"grid_search_results.csv\")\n",
    "        print(f\"✓ Copied grid search summary\")\n",
    "    \n",
    "    # Process each run in this grid search\n",
    "    for run in runs:\n",
    "        run_id = run.info.run_id\n",
    "        print(f\"\\nProcessing run: {run_id}\")\n",
    "        \n",
    "        # Run evaluation if not already done (or to ensure fresh plots)\n",
    "        best_checkpoint = Path(\"outputs/checkpoints/best_model.pt\")\n",
    "        eval_output_dir = Path(f\"outputs/evaluation/{run_id}\")\n",
    "        \n",
    "        if best_checkpoint.exists():\n",
    "            print(f\"  Running evaluation on best model...\")\n",
    "            !python -m src.evaluate.evaluate_model \\\n",
    "                --checkpoint {best_checkpoint} \\\n",
    "                --output-dir {eval_output_dir} \\\n",
    "                --device cuda\n",
    "            print(f\"  ✓ Evaluation complete\")\n",
    "        \n",
    "        # Create run-specific folder\n",
    "        run_backup_dir = backup_dir / run_id\n",
    "        run_backup_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Copy plots\n",
    "        plots_dir = Path(f\"outputs/plots/{run_id}\")\n",
    "        if plots_dir.exists():\n",
    "            shutil.copytree(plots_dir, run_backup_dir / \"plots\", dirs_exist_ok=True)\n",
    "            print(f\"  ✓ Copied plots\")\n",
    "        \n",
    "        # Copy evaluation\n",
    "        if eval_output_dir.exists():\n",
    "            shutil.copytree(eval_output_dir, run_backup_dir / \"evaluation\", dirs_exist_ok=True)\n",
    "            print(f\"  ✓ Copied evaluation\")\n",
    "        else:\n",
    "            print(f\"  ⚠ No evaluation found for {run_id}\")\n",
    "        \n",
    "        # Copy checkpoints (best_model.pt and final_model.pt)\n",
    "        checkpoints_dir = Path(\"outputs/checkpoints\")\n",
    "        if checkpoints_dir.exists():\n",
    "            checkpoint_backup = run_backup_dir / \"checkpoints\"\n",
    "            checkpoint_backup.mkdir(exist_ok=True)\n",
    "            for ckpt in [\"best_model.pt\", \"final_model.pt\"]:\n",
    "                ckpt_path = checkpoints_dir / ckpt\n",
    "                if ckpt_path.exists():\n",
    "                    shutil.copy2(ckpt_path, checkpoint_backup / ckpt)\n",
    "            print(f\"  ✓ Copied checkpoints\")\n",
    "    \n",
    "    # Copy MLflow metadata for this experiment only\n",
    "    mlflow_backup = backup_dir / \"mlruns\"\n",
    "    mlflow_backup.mkdir(exist_ok=True)\n",
    "    exp_dir = Path(f\"mlruns/{experiment.experiment_id}\")\n",
    "    if exp_dir.exists():\n",
    "        shutil.copytree(exp_dir, mlflow_backup / experiment.experiment_id, dirs_exist_ok=True)\n",
    "        print(f\"\\n✓ Copied MLflow experiment data\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"✓ All results saved to Google Drive!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nLocation: MyDrive/Schrodinger_PINN/grid_search_{timestamp}/\")\n",
    "    print(f\"  - grid_search_results.csv (summary)\")\n",
    "    print(f\"  - {len(runs)} run folders with plots, evaluations, and checkpoints\")\n",
    "    print(f\"  - mlruns/ (MLflow tracking data)\")\n",
    "else:\n",
    "    print(\"⚠ Could not find grid search experiment in MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vckR2wxT-JVj"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}